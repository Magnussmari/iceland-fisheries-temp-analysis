"""
Dataset Comparison Preparation Script
Aligns ocean temperature data with catch data for the same time period
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import sys

def load_ocean_data(file_path):
    """
    Load and process ocean temperature data from CSV
    """
    print(f"ðŸ“Š HleÃ° inn hafgÃ¶gnum frÃ¡: {file_path}")

    df = pd.read_csv(file_path, parse_dates=['time'])

    print(f"  âœ“ {len(df):,} mÃ¦lingar frÃ¡ {df['time'].min()} til {df['time'].max()}")
    print(f"  âœ“ {len(df.columns)} breytur")

    return df


def load_catch_data(file_path):
    """
    Load and process catch data from CSV
    """
    print(f"ðŸŸ HleÃ° inn aflagÃ¶gnum frÃ¡: {file_path}")

    df = pd.read_csv(file_path, parse_dates=['Dags'])

    print(f"  âœ“ {len(df):,} fÃ¦rslur frÃ¡ {df['Dags'].min()} til {df['Dags'].max()}")
    print(f"  âœ“ {df['Fisktegund'].nunique()} tegundir, {df['LÃ¶ndunarhÃ¶fn'].nunique()} hafnir")

    return df


def filter_catch_by_ocean_period(catch_df, ocean_df):
    """
    Filter catch data to match the time period of ocean data
    """
    ocean_start = pd.to_datetime(ocean_df['time'].min())
    ocean_end = pd.to_datetime(ocean_df['time'].max())

    print(f"\nðŸ” SÃ­un aflagagna fyrir tÃ­mabiliÃ°:")
    print(f"  Byrjar: {ocean_start.date()}")
    print(f"  Endar:  {ocean_end.date()}")

    # Filter catch data (convert to start of month for catch data which is monthly)
    # Get month range
    start_month = ocean_start.replace(day=1)
    end_month = ocean_end.replace(day=1)

    # Filter catch data
    filtered = catch_df[
        (catch_df['Dags'] >= start_month) &
        (catch_df['Dags'] <= end_month)
    ].copy()

    print(f"  âœ“ {len(filtered):,} aflafÃ¦rslur Ã­ sama tÃ­mabili")
    print(f"  âœ“ {filtered['Afli'].sum():,.0f} kg heildarafli")

    return filtered


def aggregate_ocean_daily(ocean_df):
    """
    Aggregate ocean data to daily averages to match catch data granularity
    """
    print(f"\nðŸ“… Ãštreikningur Ã¡ daglegu meÃ°altali fyrir hafgÃ¶gn...")

    # Create date column
    ocean_df['date'] = ocean_df['time'].dt.date

    # Select numeric columns for aggregation
    numeric_cols = ocean_df.select_dtypes(include=[np.number]).columns.tolist()

    # Aggregate by day
    daily = ocean_df.groupby('date')[numeric_cols].agg(['mean', 'std', 'min', 'max']).reset_index()

    # Flatten column names
    daily.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in daily.columns.values]
    daily = daily.rename(columns={'date': 'Dags'})
    daily['Dags'] = pd.to_datetime(daily['Dags'])

    print(f"  âœ“ {len(daily)} dagar meÃ° meÃ°altÃ¶lum")

    return daily


def aggregate_catch_daily(catch_df):
    """
    Aggregate catch data to daily totals (already monthly, but ensure daily format)
    """
    print(f"\nðŸŸ Samantekt Ã¡ aflagÃ¶gnum...")

    # Catch data is already at monthly level (Dags = first of month)
    # We'll aggregate by species and date
    daily = catch_df.groupby(['Dags', 'Fisktegund']).agg({
        'Afli': ['sum', 'mean', 'count'],
        'LÃ¶ndunarhÃ¶fn': lambda x: list(x.unique())
    }).reset_index()

    # Flatten columns
    daily.columns = ['Dags', 'Fisktegund', 'Afli_heildar', 'Afli_medal', 'Fjoldi_hafna', 'Hafnir']

    print(f"  âœ“ {len(daily)} dagsetningar meÃ° gÃ¶gnum")

    return daily


def merge_datasets(ocean_daily, catch_daily):
    """
    Merge ocean and catch data on date
    """
    print(f"\nðŸ”— SamrÃ¦ming gagnasafna...")

    # Merge on date
    merged = pd.merge(
        catch_daily,
        ocean_daily,
        on='Dags',
        how='outer',
        indicator=True
    )

    # Add merge statistics
    both_count = (merged['_merge'] == 'both').sum()
    catch_only = (merged['_merge'] == 'left_only').sum()
    ocean_only = (merged['_merge'] == 'right_only').sum()

    print(f"  âœ“ FÃ¦rslur meÃ° bÃ¡Ã°um gÃ¶gnum: {both_count}")
    print(f"  âœ“ AÃ°eins aflagÃ¶gn: {catch_only}")
    print(f"  âœ“ AÃ°eins hafgÃ¶gn: {ocean_only}")

    return merged


def create_comparison_datasets(ocean_file, catch_file, output_dir):
    """
    Main function to create aligned comparison datasets
    """
    print("=" * 80)
    print("UNDIRBÃšNINGUR SAMANBURÃARGAGNASAFNA")
    print("=" * 80)
    print(f"KeyrÃ°i: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Create output directory
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    ocean_df = load_ocean_data(ocean_file)
    catch_df = load_catch_data(catch_file)

    # Get time period from ocean data
    ocean_start = ocean_df['time'].min()
    ocean_end = ocean_df['time'].max()

    # Filter catch data to match ocean period
    catch_filtered = filter_catch_by_ocean_period(catch_df, ocean_df)

    # Aggregate to daily
    ocean_daily = aggregate_ocean_daily(ocean_df)
    catch_daily = aggregate_catch_daily(catch_filtered)

    # Merge datasets
    merged = merge_datasets(ocean_daily, catch_daily)

    # Save datasets
    print(f"\nðŸ’¾ Vista gagnasÃ¶fn...")

    # 1. Filtered catch data (original granularity)
    catch_filtered_path = output_dir / "afli_filtered_2018-02-07_to_2018-03-21.csv"
    catch_filtered.to_csv(catch_filtered_path, index=False, encoding='utf-8-sig')
    print(f"  âœ“ {catch_filtered_path.name}: {len(catch_filtered):,} fÃ¦rslur")

    # 2. Daily aggregated ocean data
    ocean_daily_path = output_dir / "ocean_daily_aggregated_2018-02-07_to_2018-03-21.csv"
    ocean_daily.to_csv(ocean_daily_path, index=False, encoding='utf-8-sig')
    print(f"  âœ“ {ocean_daily_path.name}: {len(ocean_daily):,} dagar")

    # 3. Daily aggregated catch data
    catch_daily_path = output_dir / "afli_daily_aggregated_2018-02-07_to_2018-03-21.csv"
    catch_daily.to_csv(catch_daily_path, index=False, encoding='utf-8-sig')
    print(f"  âœ“ {catch_daily_path.name}: {len(catch_daily):,} fÃ¦rslur")

    # 4. Merged dataset
    merged_path = output_dir / "ocean_catch_merged_2018-02-07_to_2018-03-21.csv"
    merged.to_csv(merged_path, index=False, encoding='utf-8-sig')
    print(f"  âœ“ {merged_path.name}: {len(merged):,} fÃ¦rslur")

    # Generate statistics
    print(f"\nðŸ“Š TÃ¶lfrÃ¦Ã°i fyrir sameinaÃ° gagnasafn:")

    # Ocean temperature stats
    if 'sbe38_bow_temperature_mean' in merged.columns:
        temp_col = 'sbe38_bow_temperature_mean'
        temp_stats = merged[temp_col].describe()
        print(f"\n  SjÃ¡varhitastig (K):")
        print(f"    MeÃ°altal: {temp_stats['mean']:.2f}K ({temp_stats['mean']-273.15:.2f}Â°C)")
        print(f"    LÃ¡gmark:  {temp_stats['min']:.2f}K ({temp_stats['min']-273.15:.2f}Â°C)")
        print(f"    HÃ¡mark:   {temp_stats['max']:.2f}K ({temp_stats['max']-273.15:.2f}Â°C)")

    # Catch stats
    if 'Afli_heildar' in merged.columns:
        catch_stats = merged['Afli_heildar'].describe()
        print(f"\n  Afli (kg):")
        print(f"    Heildarafli: {merged['Afli_heildar'].sum():,.0f} kg")
        print(f"    MeÃ°altal/dag: {catch_stats['mean']:,.0f} kg")
        print(f"    HÃ¡mark/dag: {catch_stats['max']:,.0f} kg")

    print("\n" + "=" * 80)
    print("âœ“ UNDIRBÃšNINGUR LOKIÃ")
    print("=" * 80)
    print(f"\nGagnasÃ¶fn vistuÃ° Ã­: {output_dir}")

    return {
        'catch_filtered': catch_filtered,
        'ocean_daily': ocean_daily,
        'catch_daily': catch_daily,
        'merged': merged,
        'output_dir': output_dir
    }


if __name__ == "__main__":
    # Paths
    script_dir = Path(__file__).parent
    project_root = script_dir.parent

    # Input files
    ocean_file = project_root / "data/processed/oceantemp/Ocean_temp_FULL.csv"
    catch_file = project_root / "data/processed/afli_eftir_fisktegundum/afli_hreinsad_FULL.csv"

    # Output directory
    output_dir = project_root / "data/processed/comparison"

    # Check if files exist
    if not ocean_file.exists():
        print(f"âŒ Villa: HafgagnaskrÃ¡ fannst ekki: {ocean_file}")
        sys.exit(1)

    if not catch_file.exists():
        print(f"âŒ Villa: AflaskrÃ¡ fannst ekki: {catch_file}")
        sys.exit(1)

    # Create comparison datasets
    results = create_comparison_datasets(ocean_file, catch_file, output_dir)

    print(f"\nðŸ’¡ Til aÃ° skoÃ°a gÃ¶gnin:")
    print(f"   import pandas as pd")
    print(f"   df = pd.read_csv('{results['output_dir']}/ocean_catch_merged_2018-02-07_to_2018-03-21.csv')")
    print(f"   print(df.head())")
